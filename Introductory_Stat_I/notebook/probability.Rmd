---
title: "Probabilidade"
author: "Fernando B. Sabino da Silva"
output:
  slidy_presentation:
    css: "https://asta.math.aau.dk/course/asta/2018-1/?file=lecture_style.css"
    fig_caption: no
    highlight: tango
    theme: cerulean
  pdf_document:
    fig_caption: no
    keep_tex: yes
    highlight: tango
    number_section: yes
    toc: yes
---

```{r, include = FALSE}
## Remember to add all packages used in the code below!
missing_pkgs <- setdiff(c("mosaic", "pander", "VennDiagram"), rownames(installed.packages()))
if(length(missing_pkgs)>0) install.packages(missing_pkgs)
library(VennDiagram)
```

# Probabilidade de eventos

## O conceito de probabilidade

* Experimento: Medir o tempo de espera em uma fila. Se o tempo exceder 2 minutos marque $1$ e 0, caso contrário.
* O experimento é realizado $n$ vezes com resultados $y_1,y_2,\ldots,y_n$. Há uma **variação aleatória** no resultado, i.e. às vezes ocorre $1$ e em outras vezes ocorre 0.
* **Probabilidade empírica** de exceder 2 minutos:\
    $$ 
    p_n = \frac{\sum_{i=1}^n y_i}{n}.
    $$
* **Probabilidade teórica** de exceder 2 minutos:\ 
    $$
    \pi = \lim_{n\rightarrow\infty} p_n.
    $$.
* Tentamos inferir o verdadeiro valor de $\pi$ com base em uma amostra.
  Por exemplo, "$\pi > 0.1$?" 
  ("mais de 10\% dos clientes experimentaram um tempo de espera superior a 2 minutos?").
* A (inferência) estatística está preocupada com tais questões que são úteis para a tomada de decisões. Em geral, só temos acesso a uma amostra finita.



## Experimento real

* Em um determinado mês de 2017,\ um grupo de estudantes respondeu a pergunta de quanto tempo eles precisaram esperar na fila em uma determinada cantina (em minutos):
```{r}
y_cantina <- c(2, 5, 1, 6, 1, 1, 1, 1, 3, 4, 1, 2, 1, 2, 2, 2, 4, 2, 2, 5, 20, 2, 1, 1, 1, 1)
x_cantina <- ifelse(y_cantina > 2, 1, 0)
x_cantina
```
* Probabilidade empírica de esperar mais de 2 minutos:
```{r}
p_cantina <- sum(x_cantina) / length(x_cantina)
p_cantina
```
* Questão: A probabilidade na população é  $\pi > 1/3$?
* Nota: Um estudante disse que esperou por 20 minutos. Dado os outros resultados, podemos duvidar da veracidade e ignorar essa observação).

```{r, include = FALSE}
# set.seed(1)
#y<-y_canteen
# y_ref <- replicate(1000, sample(y, replace = TRUE))
# p_ref <- apply(y_ref, 2, function(x) mean(x > 2))
# quantile(p_ref, c(0.025, 0.975))
# 
# f <- function(x) quantile(p_ref, x, names = FALSE) - 1/3
# 1 - uniroot(f, c(0, 1))$root
# mean(p_ref > 1/3)
```



## Outro experimento

* John Kerrich, um matemático sul-africano, estava visitando Copenhague quando a segunda Guerra Mundial eclodiu. Dois dias antes ele estava programado para viajar para a Inglaterra quando os alemães invadiram a Dinamarca. Kerrich passou o resto da guerra em um acampamento e para passar o tempo realizou uma série de experimentos. Em um destes, ele jogou uma moeda 10,000 vezes. Os resultados são mostrados a seguir.
* Abaixo, `x` é um vetor com os primeiros 2000 resultados do experimento de John Kerrich.
(0 = coroa,\ $1$ = cara):
```{r, include = FALSE}
x <- c(0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 
0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 
0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 
1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 
0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 
1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 
0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 
1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 
1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 
0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 
1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 
1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 
1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 
0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 
0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 
1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 
1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 
0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 
0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 
1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 
1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 
1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 
1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 
0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 
1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 
0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 
0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 
1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 
1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 
0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 
0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 
1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 
1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 
0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 
0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 
0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 
1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 
1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 
0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 
0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 
1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 
1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 
0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 
0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 
0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 
0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 
1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 
0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 
1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 
0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 
0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 
0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 
0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 
0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 
0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 
0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 
1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 
1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 
0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 
0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 
1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 
0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 
1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 
0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 
0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 
1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 
0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 
1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 
0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 
1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 
0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 
1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 
1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 
0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 
1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 
0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 
0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 
0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 
1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 
0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 
0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 
1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 
1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 
0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 
0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 
0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 
1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 
0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 
0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 
1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 
1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 
0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 
1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 
1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 
1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 
0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 
1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 
1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 
1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 
0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 
1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 
1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 
1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 
0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 
1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 
0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 
1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 
0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 
1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 
0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 
0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 
1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 
1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 
0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 
0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 
1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 
0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 
0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 
1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 
1)
```
```{r}
head(x, 10)
```
* Gráfico da probabilidade empírica $p_n$ de sair cara contra o número de lançamentos $n$:

```{r, echo=FALSE, fig.height = 5, fig.width = 6, fig.align = "center"}
plot(cumsum(x)/seq_along(x), type = "l", log = "x", xlab = "n", ylab = expression(p[n]))
abline(h = 0.5, lty = 2)
```

(O eixo horizontal está na escala log).



## Definições

* **Espaço Amostral**:\ Todos os possíveis resultados de um experimento.
* **Evento**:\ Qualquer subconjunto do espaço amostral.

Nós conduzimos o experimento $n$ vezes. Seja $\#(A)$ o número de vezes que observamos o evento $A$.

* **Probabilidade empírica** do evento $A$:
  $$
  p_n(A)=\frac{\#(A)}{n}.
  $$
* **Probabilidade teórica** do evento $A$:
  $$
  P(A)=\lim_{n\to\infty}p_n(A)
  $$
<!-- * Nota: $0\leq P(A)\leq 1$. -->



## Probabilidades teóricas de dois eventos

* Se dois eventos $A$ e $B$ são **disjuntos** (sem intersecção) então  
    + $\#(A \text{ e } B) = 0$ implica que $P(A \cap B)=0.$
    + $\#(A \text{ ou } B)=\#(A)+\#(B)$ implica que $P(A \cup B)=P(A)+P(B).$
  
```{r, echo = FALSE, fig.height = 3.5, fig.width = 4.5}
venn.plot <- draw.pairwise.venn(area1 = 70, area2 = 70, cross.area = 0, 
                                category = c("A", "B"), cex = 0, cat.cex = 4)
grid.draw(venn.plot)
grid.newpage()
```

* Se dois eventos $A$ e $B$ **não são disjuntos** então a fórmula mais geral é
  $$
  P(A \cup B) = P(A) + P(B) - P(A \cap B).
  $$

```{r, echo = FALSE, fig.height = 3.5, fig.width = 4.5}
venn.plot <- draw.pairwise.venn(area1 = 70, area2 = 70, cross.area = 30, 
                                category = c("A", "B"), cex = 0, cat.cex = 4)
grid.draw(venn.plot)
grid.newpage()
```

## Probabilidade Condicional

* Digamos que consideramos dois eventos $A$ e $B$.\ Então a **probabilidade condicional** de $A$ dado 
(ou condicional ao) o evento $B$ é escrito $P(A \mid B)$ e é definido por
$$P(A \mid B)=\frac{P(A \cap B)}{P(B)}.$$
* A probabilidade acima pode ser entendida do seguinte modo: "quão provável é o evento $A$ se nós sabemos que $B$ ocorreu". 

---- 

### Exemplo com os dados das revistas:
```{r message=FALSE}
magAds <- read.delim("C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/datasets_ads.txt")

# Criando dois novos fatores: 'words' e 'education':
magAds$words <- cut(magAds$WDS, breaks = c(31, 72, 146, 230), include.lowest = TRUE)
magAds$education <- factor(magAds$GROUP, levels = c(1, 2, 3), labels = c("high", "medium", "low"))

library(mosaic)
tab <- tally( ~ words + education, data = magAds)
tab
```

* O evento $A$=$\{$words=`r levels(magAds$words)[3]`$\}$ (o anúncio é um texto "difícil") tem probabilidade empírica
$$ p_n(A) = \frac{9 + 6 + 5}{54} = \frac{20}{54} \approx 37 \%.$$
* Digamos que só estamos interessados na probabilidade de um texto "difícil" (evento $A$) para revistas de educação superior (high education), i.e. condicionando no evento         $B=\{$education=high$\}$. Então a probabilidade condicional empírica pode ser calculada a partir da tabela acima:
  
  $$
  p_n(A \mid B) = \frac{9}{4+5+9} = \frac{9}{18} = 0.5 = 50\%.
  $$
  
* A probabilidade condicional de $A$ dado $B$ pode ser (teoricamente) expressada por

  $$
  \begin{aligned}
  P(A \mid B) 
  &= P(\text{words} =`r levels(magAds$words)[3]` \mid \text{education = high})  \\[0.5em]
  &= \frac{P(\text{words} =`r levels(magAds$words)[3]` \cap \text{education = high})}{P(\text{education = high})},  \\
  \end{aligned}
  $$
  que traduzindo para a probabilidade empírica (substituindo $P$ por $p_n$) dará

  $$
  \begin{aligned}
  p_n(A \mid B) 
  &= \frac{p_n(\text{words} =`r levels(magAds$words)[3]` \cap \text{education = high})}{p_n(\text{education = high})}  \\
  &= \frac{\frac{9}{54}}{\frac{4+5+9}{54}} \\
  &= \frac{9}{4+5+9} \\[0.5em]
  &= 50\%
  \end{aligned}
  $$
  como calculado acima.

## Probabilidade condicional e independência
* Se a informação sobre $B$ não muda a probabilidade de $A$ nós dizemos que $A$ é **independente** de $B$ e escrevemos
  $$
  P(A \mid B) = P(A) \quad \Leftrightarrow  \quad P(A \cap B) = P(A)P(B)
  $$
  Quando isto acontecer nós dizemos que
  $A$ e $B$ são **eventos independentes**.
* Em geral, os eventos $A_1, A_2, ..., A_k$ são independentes se
  $$
  P(A_1 \cap A_2 \cap ... \cap A_k) = P(A_1) P(A_2) \cdots P(A_k)
  $$
  e se o produto de todas as combinações de ordem 2 a k-1 também são válidas, i.e., k eventos são ditos independentes se são **independentes k a k, (k-1) a (k-1),..., dois a dois**, isto é, para cada subconjunto $S$ de ${1, 2,\ldots k}$.
  
### Dados das revistas revisitados
* Lembre-se das probabilidades empíricas calculadas acima:
$$
p_n(A) = 37 \%
\quad \text{e} \quad
p_n(A \mid B) = 50\%.
$$
* Isto indica (não podemos dizer com certeza, pois só temos a disposição uma amostra finita - no curso de estatística II vemos detalhes de como testar isso) que a probabilidade teórica

$$
P(A) \neq P(A \mid B)
$$
e, portanto, que o conhecimento sobre o evento $B$ (nível de educação elevado) pode transmitir informações sobre a probabilidade do evento $A$ (o anúncio contém um texto "difícil").

## Um pouco mais de formalidade
  
### Modelos de Probabilidade
Quando discutimos modelos de probabilidade, nós falamos de **experimentos** aleatórios que produzem um dos vários **resultados** possíveis. Um **modelo de probabilidade** que descreve a incerteza de um experimento consiste em três elementos (descrevemos dois abaixo):

* O **espaço amostral**, geralmente denominado por $\Omega$, representando o conjunto que contém todos os resultados possíveis.
* Uma **função de probabilidade** que atribui a um evento $A$ um número não-negativo, $P[A]$, que representa a probabilidade de que o evento $A$ ocorra como resultado do experimento.

Nós chamamos de $P[A]$ a **probabilidade** do evento $A$. Um evento $A$ pode ser qualquer subconjunto do espaço amostral, não necessariamente um único resultado possível. As leis da probabilidade devem seguir uma série de regras, que são o resultado de um conjunto de axiomas que introduziremos agora.

## Axiomas de Probabilidade

Dado um espaço amostral $\Omega$ para um experimento em particular, a **função de probabilidade** associada ao experimento deve satisfazer os seguintes axiomas.

  + *Não-negatividade*: $P[A] \geq 0$ para qualquer evento $A \subset\Omega$.
  + *Normalização*: $P[\Omega] = 1$. Ou seja, a probabilidade de todo o espaço é $1$.
  + *Aditividade*: Para eventos mutuamente exclusivos $E_1, E_2,\ldots$ $$P\left[\bigcup_{i = 1}^{\infty} E_i\right] = \sum_{i = 1}^{\infty} P[E_i]$$.
  
  Usando estes axiomas, muitas regras adicionais de probabilidade podem ser facilmente derivadas.
  

## Regras de Probabilidade

* Dado um evento $A$ e seu complemento, $A^c$, ou seja, os resultados em $\Omega$ que não estão em $A$, temos a regra do **complementar**:

$$P[A^c] = 1 - P [A]$$

* Em geral, para dois eventos $A$ e $B$, temos a **regra da adição**:

$$P[A \cup B] = P[A] + P[B] - P[A \cap B]$$
*Exercício: Prove que as relações acima são válidas usando os três axiomas de probabilidade. Prove também que se* $A\subseteq B$, *então* $P(A)\leq P(B)$. 
    
* Se $A$ e $B$ também são *disjuntos*, então temos:

$$ P[A \cup B] = P[A] + P[B] $$
Se tivermos $n$ eventos mutuamente exclusivos, $E_1, E_2,\ldots E_n$, então temos:

$$ P\left[\textstyle \bigcup_{i = 1}^{n} E_i \right] = \sum_{i = 1}^{n} P[E_i] $$

## Regra de Bayes

Defina uma **partição** de um espaço amostral $\Omega$ como um conjunto de eventos disjuntos $A_1, A_2, \ldots, A_n$ cuja união é o espaço amostral $\Omega$. Isso é

$$A_i \cap A_j = \emptyset $$

para todos $i \neq j$ e

$$ \bigcup_{i = 1}^{n} A_i = \Omega. $$

Seja $A_1, A_2, \ldots, A_n$ formam uma partição do espaço amostral, onde $P[A_i]>0$ para todo $i$. Então para qualquer evento $B$ com $P[B]>0$ temos a **Regra de Bayes**:

$$ P[A_i | B] = \frac{P[A_i] P[B | A_i]} {P[B]} = \frac{P[A_i] P[B | A_i]}{\sum_{i = 1}^{n} P[A_i] P[B | A_i]} $$

O denominador da última igualdade é freqüentemente chamado de **lei da probabilidade total**:

$$ P[B] = \sum_{i = 1}^{n} P[A_i] P[B | A_i] $$

Dois eventos $A$ e $B$ são considerados **independentes** se satisfizerem

$$ P[A \cap B] = P[A] \cdot P[B] $$

Uma coleção de eventos $E_1, E_2, \ldots E_n$ é considerada independente se

$$ P\left[\bigcap_{i\in S} E_i \right] = \prod_{i \in S} P[E_i] $$

para cada subconjunto $S$ de ${1, 2,\ldots n}$.

## Distribuição discreta
### Exemplo: Dados das revistas
```{r size="small",tidy=FALSE,message=FALSE}
# Tabela contendo o percentual de anúncios em cada combinação dos níveis de 'words' e 'education'
tab <- tally( ~ words + education, data = magAds, format = "percent")
round(tab, 2) # Duas casas decimais
```

* Os 9 eventos disjuntos acima (correspondentes as combinações de `words` e
  `education`) compõem todo o espaço amostral para as duas variáveis. As probabilidades empíricas de cada evento são dadas na tabela.
  
### Distribuição discreta
* Em geral:
    + Seja $A_1,A_2,\ldots,A_k$ uma subdivisão do espaço amostral em 
    eventos disjuntos (par a par).
    + As probabilidades $P(A_1), P(A_2), ..., P(A_k)$ (**distribuição discreta**) satisfazem
    $$\sum_{i=1}^kP(A_i)=1.$$
    
----


### Exemplo: Três lançamentos de uma moeda 

* **Variável aleatória/estocástica**: Uma função $Y$ que.
* Possible outcomes in an experiment with 3 coin tosses:

* **Variável aleatória**: Uma variável aleatória é simplesmente uma função $Y$ que mapeia os resultados do espaço amostral para números reais, isto é, que mapeia os possíveis resultados do experimento em um número.
* Resultados possíveis de um experimento com 3 lançamentos de moedas:
    + 0 caras (KKK)
    + 1 cara (CKK, KCK, KKC)
    + 2 caras (CCK, CKC, KCC)
    + 3 caras (CCC)
* Os eventos acima são disjuntos e compõem todo o espaço amostral.
* Seja $Y$ o número de caras no experimento: $Y(KKK) = 0, Y(CKK) = 1, \ldots$
* Assuma que cada resultado é igualmente provável, i.e. 1/8 de probabilidade para cada um deles.
  Então,
    + $P(\text{nenhuma cara}) = P(Y = 0) = P(KKK) = 1/8.$
    + $P(\text{uma cara}) = P(Y = 1) = P(CKK \text{ ou } KCK \text{ ou } KKC) = P(CKK) + P(KCK) + P(KKC) = 3/8.$
    + Similarmente para 2 ou 3 caras.
* Então, a distribuição de $Y$ é

  Número de caras, $Y$   0   1   2   3
  --------------------- --- --- --- ---
  Probabilidade         1/8 3/8 3/8 1/8



# Distribuição de variáveis aleatórias

## Distribuição de probabilidade
* Exemplo: Conduzimos um experimento no qual fazemos uma medição quantitativa $Y$ (uma variável aleatória), por exemplo, contamos o número de palavras em um anúncio ou o tempo de espera em uma fila.
* De antemão, há muitos resultados possíveis para os experimentos, i.e. os valores de $Y$ que irão acontecer em uma realização do experimento são incertos, mas nós podemos quantificá-los pela **distribuição de probabilidade** de $Y$.

* Uma definição não rigorosa, mas útil para transmitir a ideia é:
$$ \text{distribuição} = \text{lista de possíveis} \textbf{ valores} + \text{probabilidades} \textbf{ associadas} $$

* Para qualquer intervalo $(a, b$), a distribuição indica a probabilidade de observar um valor da variável aleatória $Y$ neste intervalo:
  $$ P(a<Y<b),\qquad -\infty < a < b < \infty.$$
* Se os possíveis valores de uma variável aleatória são discretos, isto é, se nós podemos enumerar todos os possíveis valores de $Y$, então a variável aleatória $Y$ é chamada de **discreta**. Por exemplo, o número de palavras em um anúncio. 
* Se os possíveis valores de uma variável aleatória são contínuos, isto é, $Y$ pode assumir qualquer valor dentro de um intervalo, então a variável aleatória $Y$ é chamada de **contínua**. Por exemplo, o tempo de espera em uma fila.

## Variáveis Aleatórias Discretas
* A distribuição de uma variável aleatória discreta $X$ é mais frequentemente especificada por uma lista de possíveis valores e uma função massa de probabilidade $p(x)$, isto é, 

$$ p(x) = p_X(x) = P[X = x]. $$
Costumeiramente nós abandonamos o subscrito da notação mais correta $p_X(x)$ e simplesmente escrevemos $p(x)$. A variável aleatória relevante será discernida do contexto.

O exemplo mais comum de uma variável aleatória discreta é a variável aleatória com distribuição binomial. A função massa de uma variável aleatória $X$ com distribuição binomial é dada por

$$ p(x | n, p) = {n \choose x} p^x(1 - p)^{n - x}, \ \ \ x = 0, 1, \ldots, n, \ n \in \mathbb{N}, \ 0 < p < 1. $$

A última linha contém uma grande quantidade de informação.

* A função $p(x | n, p)$ é a função massa. É uma função de $x$, os possíveis valores da variável aleatória $X$. Ela é condicional aos parâmetros $n$ e $p$. Valores diferentes destes parâmetros especificam distribuições binomiais diferentes.

* $x = 0, 1, \ldots, n$ indica o espaço amostral de $x$, isto é, os possíveis valores da variável aleatória.

$n \in \mathbb{N}$ e $0 < p < 1$ especificam os espaços dos parâmetros. Estes são os valores possíveis dos parâmetros que fornecem uma distribuição binomial válida. Frequentemente, toda essa informação é simplesmente codificada escrevendo

$$ X \sim \text{bin}(n, p). $$

## Variáveis Aleatórias Contínuas
* A distribuição de uma variável aleatória contínua $X$ é mais frequentemente especificada pelo conjunto de possíveis valores e uma função densidade de probabilidade, $f(x)$. (A função de distribuição acumulada (densidade acumulada), $F(x)$ ou a função característica ou geratriz de momentos também seriam suficientes.)

* A probabilidade do evento $a < X < b$ é calculada por

$$ P[a < X < b] = \int_{a}^{b} f(x)dx. $$

Note que as densidades não são probabilidades.

* O exemplo mais comum de uma variável aleatória contínua é uma variável aleatória com distribuição normal. A densidade de uma variável aleatória normal $X$, é dada por

$$ f(x | \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left[\frac{-1}{2} \left(\frac{x - \mu}{\sigma}\right)^2 \right], \ \ \ -\infty < x < \infty, \ -\infty < \mu < \infty, \ \sigma > 0. $$

* A função $f(x | \mu, \sigma^2)$ é a função de densidade. É uma função de  $x$, os valores possíveis da variável aleatória $X$. Ela é condicional aos parâmetros $\mu$ e $\sigma^2$. Diferentes valores destes parâmetros especificam diferentes distribuições normal.
* $-\infty < x < \infty$ indica o espaço amostral de $x$. Neste caso, a variável aleatória pode assumir todo e qualquer valor real.
* $-\infty < \mu < \infty$ and $\sigma > 0$ especificam o espaço de parâmetros. Estes são os valores possíveis dos parâmetros que fornecem uma distriuição normal válida. Muitas vezes, toda essa informação é simplesmente codificada escrevendo

$$ X \sim N(\mu, \sigma^2) $$

## Independência entre Variáveis Aleatórias
Considere duas variáveis aleatórias $X$ e $Y$. Nós dizemos que elas são independentes se

$$ f(x, y) = f(x) \cdot f(y) $$

para todo $x$ e $y$. Aqui $f(x, y)$ é a função densidade (massa) conjunta de $X$ e $Y$. Nós chamamos de $f(x)$ e de $f(y)$ as funções densidade (massa) marginais de $X$ e de $Y$, respectivamente. 

* A função densidade (massa) conjunta $f(x, y)$ juntamente com os possíveis valores $(x, y)$ especificam a distribuição conjunta de $X$ e $Y$.

Noções similares existem para mais de duas variáveis aleatórias.
    
### Amostra Aleatória

  Nós realizamos um experimento $n$ vezes, onde o resultado do $i$-ésimo
  experimento corresponde a uma medição de uma variável aleatória $Y_i$,
  onde assumimos que

* Os experimentos são **independentes**
* As variáveis $Y_1,\ldots,Y_n$ têm a **mesma distribuição**

## Parâmetros populacionais

* Quando o tamanho da amostra aumenta, a média da amostra, $\overline{y}$, por exemplo, irá se estabilizar em torno de um valor fixo, $\mu$, que é usualmente desconhecido. O valor $\mu$ é chamado de **média populacional**.
* Correspondingly, the standard deviation of the sample, $s$, will stabilize around a fixed value, $\sigma$, which is usually unknown. The value $\sigma$ is called the **population standard deviation**.
* Notation:
	+ $\mu$ (mu) denotes the population mean.
	+ $\sigma$ (sigma) denotes the population standard deviation.

| Population | Sample       |
|:----------:|:------------:|
|$\mu$       |$\overline{y}$|
|$\sigma$    |$s$           |

----

### Distribution of a discrete random variable

* Possible values for $Y$:\ $\{y_1,y_2,\ldots,y_k\}$.
* The **distribution** of $Y$ is the probabilities of each possible value:\ $p_i=P(Y=y_i), \quad i=1,2,\ldots,k$.
* The distribution satisfies: $\sum_{i=1}^kp_i=1$.

Expectations
For discrete random variables, we define the expectation of the function of a random variable $X$ as follows.

$$ \mathbb{E}[g(X)] \triangleq \sum_{x} g(x)p(x) $$

For continuous random variables we have a similar definition.

$$ \mathbb{E}[g(X)] \triangleq \int g(x)f(x) dx $$

For specific functions $g$, expectations are given names.

The mean of a random variable $X$ is given by

$$ \mu_{X} = \text{mean}[X] \triangleq \mathbb{E}[X]. $$

So for a discrete random variable, we would have

$$ \text{mean}[X] = \sum_{x} x \cdot p(x) $$

For a continuous random variable we would simply replace the sum by an integral.

The variance of a random variable $X$ is given by

$$ \sigma^2_{X} = \text{var}[X] \triangleq \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2. $$

The standard deviation of a random variable $X$ is given by

$$ \sigma_{X} = \text{sd}[X] \triangleq \sqrt{\sigma^2_{X}} = \sqrt{\text{var}[X]}. $$

The covariance of random variables $X$ and $Y$ is given by

$$ \text{cov}[X, Y] \triangleq \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X] \cdot \mathbb{E}[Y]. $$

Likelihood
Consider $n$ iid random variables $X_1, X_2, \ldots X_n$. We can then write their likelihood as

$$ \mathcal{L}(\theta \mid x_1, x_2, \ldots x_n) = \prod_{i = i}^n f(x_i; \theta) $$

where $f(x_i; \theta)$ is the density (or mass) function of random variable $X_i$ evaluated at $x_i$ with parameter $\theta$.

Whereas a probability is a function of a possible observed value given a particular parameter value, a likelihood is the opposite. It is a function of a possible parameter value given observed data.

Maximumizing likelihood is a common techinque for fitting a model to data.

## Expected value (mean) for a discrete distribution

* The **expected value** or **(population) mean** of $Y$ is
  $$
      \mu = \sum_{i=1}^k y_ip_i
  $$
* An important property of the expected value is that it has the same unit as the observations (e.g. meter).

### Example: number of heads in 3 coin flips
* Recall the distribution of $Y$ (number of heads):

      | y (number of heads) | 0 | 1 | 2 | 3 |
      |:--------------------|:-:|:-:|:-:|:-:|
      |     $P(Y = y)$      |1/8|3/8|3/8|1/8|

* Then the expected value is

  $$
  \mu = 0\frac{1}{8}+1\frac{3}{8}+2\frac{3}{8}+3\frac{1}{8}=1.5.
  $$

  *Note that the expected value is not a possible outcome of the experiment itself.*



## Variance and standard deviation  for a discrete distribution

* The **(population) variance** of $Y$ is
    $$
        \sigma^2 = \sum_{i=1}^k (y_i - \mu)^2 p_i
    $$
* The **(population) standard deviation** is $\sigma = \sqrt{\sigma^2}$.
* Note: If the observations have unit meter, the **variance** has unit $\text{meter}^2$ which is hard to interpret. The **standard deviation** on the other hand has the same unit as the observations (e.g. meter).

### Example: number of heads in 3 coin flips

The distribution of the random variable 'number of heads in 3 coin flops' has variance
    $$ \sigma^2
    = (0-1.5)^2\frac{1}{8} + (1-1.5)^2\frac{3}{8} + (2-1.5)^2 \frac{3}{8} + (3-1.5)^2 \frac{1}{8} = 0.75.
    $$

and standard deviation
    $$  \sigma = \sqrt{\sigma^2} = \sqrt{0.75} = 0.866.  $$


## The binomial distribution


* The **binomial distribution** is a discrete distribution
* The distribution occurs when we conduct a
    success/failure experiment $n$ times with probability $\pi$ for
    success. If $Y$ denotes the number of successes it can be shown that
    $$p_Y(y) = P(Y = y) = \binom{n}{y} \pi^y (1-\pi)^{n-y},$$
    where $\binom{n}{y}=\frac{n!}{y!(n-y)!}$ and $m!$ is the product of the
    first $m$ integers.
* Expected value: $\mu = n \pi$.
* Variance: $\sigma^2 = n \pi (1-\pi)$.
* Standard deviation: $\sigma = \sqrt{n \pi (1-\pi)}$.


```{r dbinom}
# The binomial distribution with n = 10 and pi = 0.35:
plotDist("binom", size = 10, prob = 0.35,
         ylab = "Probability", xlab = "Number of successes", main = "binom(n = 10, prob = 0.35)")
```



## Distribution of a continuous random variable

* The distribution of a continuous random variable $Y$ is characterized by the so-called
  probability density function $f_Y$.

```{r normprobs,echo=FALSE,fig.width=6,fig.height=4}
x <- (-70:70)/20
par(mar=c(3,0,0,0))
plot(x, dnorm(x), axes=F, xlab="", ylab="", type="l", ylim=c(-.01,.4), main="")
abline(h=0)
lines(c(0,0),c(0,dnorm(0)))
lines(c(1.5,1.5),c(0,dnorm(1.5)))
x <- (0:30)/20
y <- c(0,dnorm(x),0)
polygon(c(0,x,1.5),y,density=20)#,col="cyan")
axis(1,at=0,labels="a",pos=0,cex.axis=1.5)
axis(1,at=1.5,labels="b",pos=0,cex.axis=1.5)
```

* The area under the graph of the probability density function between $a$ and $b$
  is equal to the probability of an observation in this interval.
* $f_Y(y)\geq 0$ for all real numbers $y$.
* The area under the graph for $f_Y$ is equal to 1.
* For example the **uniform distribution** from $A$ to $B$:
    $$
    f_Y(y)=
    \begin{cases}
      \frac{1}{B-A} & A<y<B \\
      0 & \text{otherwise}
    \end{cases}
    $$
```{r unifdist,echo=FALSE,fig.width=6,fig.height=4}
par(mar=c(3,3,0,0))
plot(c(1,2,2,4,4,5)-1, c(0,0,.5,.5,0,0), axes=F, xlab="", ylab="", type="l", main="", lwd = 3, ylim = c(0,.6))
lines(c(0,0),c(0,.6))
lines(c(0,4.1),c(0,0))
axis(1,at=1,labels="A",pos=0,cex.axis=1.5)
axis(1,at=3,labels="B",pos=0,cex.axis=1.5)
axis(2,at=0,pos=0,cex.axis=1.5)
axis(2,at=.5,labels=expression(frac(1,B-A)),pos=0,cex.axis=1.5)
```


## Density function
### Increasing number of observations

* Another way to think about the density is in terms of the histogram.
* If we draw a histogram for a sample where the area of each box corresponds to the relative frequency of each interval, then the total area will be 1.
* When the number of observations (sample size) increase we can make a finer interval division and get a more smooth histogram.
* We can imagine an infinite number of observations, which would produce a nice smooth curve, where the area below the curve is $1$.\ A function derived this way is also what we call the **probability density function**.

```{r histToPop,echo=FALSE,results='hide',fig.width=10,fig.height=4}
par(mfrow=c(1,3),cex.main = 2,cex.lab = 2,mar=c(5,5,4,1))
set.seed(100)
varValue <- rnorm(50,10,2)
hist(varValue,breaks="FD",ylab="Density",xlab = "y",ylim=c(0,.25),freq=F,main="Histogram of 50 obs.")
text(7,.22,bquote(bar(y) == .(round(mean(varValue),2))),cex=1.5)
text(14,.22,bquote(s == .(round(sd(varValue),2))),cex=1.5)
varValue <- rnorm(1000,10,2)
hist(varValue,breaks="FD",freq=F,ylab="Density",xlab = "y",ylim=c(0,.25),main="Histogram of 500 obs.")
text(7,.22,bquote(bar(y) == .(round(mean(varValue),2))),cex=1.5)
text(14,.22,bquote(s == .(round(sd(varValue),2))),cex=1.5)
varValue <- (20:180)/10
plot(varValue,dnorm(varValue,10,2),ylab="Density",xlab = "y",ylim=c(0,.25),type="l",main="Histogram of population")
text(7,.22,bquote(mu == 10),cex=1.5)
text(14,.22,bquote(sigma == 2),cex=1.5)
```

----

### Density shapes

```{r densities, echo=FALSE, results='hide', fig.width=8, fig.height=4, out.width='\\textwidth'}
par(mfrow=c(2,2), cex.lab = 1, cex.main = 1, mar=c(1,5,4,1))

#x <- (0:200)/100
#y <- .5+(x-1)^2
#plot(x,y,axes=F,type="l",ylab="Density",xlab = "")

x <- seq(0, 1, length.out = 100)
plot(x,dbeta(x, 1/2, 1/2),axes=F,type="l",ylab="Density",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Symmetric density\n U-shaped")

x <- (0:200)/100
plot(x,dnorm(x,1,.35),axes=F,type="l",ylab="Density",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Symmetric density\n Bell-shaped")

x <- (0:400)/100
plot(x,dgamma(x,1.5,1.5),axes=F,type="l",ylab="Density",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Right skew density")

plot(x,dgamma(rev(x),1.5,1.5),axes=F,type="l",ylab="Density",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Left skew density")
```

## Normal distribution

* The normal distribution is a continuous distribution determined by two parameters:
    * $\mu$: the **mean** (expected value), which determines
        where the distribution is centered.
    * $\sigma$: the **standard deviation**, which determines
        the spread of the distribution about the mean.
* The distribution has a bell-shaped probability density function:
    $$ f_Y(y;\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp \left (-\frac{1}{2\sigma^2}(y-\mu)^2 \right ) $$
* When a random variable $Y$ follows a normal distribution with mean $\mu$ and standard
  deviation $\sigma$, then we write $Y \sim \texttt{norm}(\mu,\sigma)$.
* We call $\texttt{norm}(0, 1)$ the **standard normal distribution**.

----

### Reach of the normal distribution

```{r normalreach,echo=FALSE, fig.width=10, fig.height=5}
x <- (-70:70)/20
par(xpd=TRUE)
plot(x,dnorm(x),axes=F,xlab="",ylab="",type="l",ylim=c(-.21,.4), main="Density of the normal distribution", cex.main = 1.5)
axis(1,at=-3:3,labels=F,pos=0)
axis(1,at=-3,labels=substitute(mu-3*sigma),pos=0)
axis(1,at=-2,labels=substitute(mu-2*sigma),pos=0)
axis(1,at=-1,labels=substitute(mu-sigma),pos=0)
axis(1,at=0,labels=substitute(mu),pos=0)
axis(1,at=1,labels=substitute(mu+sigma),pos=0)
axis(1,at=2,labels=substitute(mu+2*sigma),pos=0)
axis(1,at=3,labels=substitute(mu+3*sigma),pos=0)
arrows(-1,-.1,1,-.1,col="red",code=3,length=.1)
text(-.01,-.115,"68%",col="red")
arrows(-2,-.15,2,-.15,col="blue",code=3,length=.1)
text(-.01,-.1655,"95%",col="blue")
arrows(-3,-.2,3,-.2,col="green",code=3,length=.1)
text(-.01,-.215,"99.7%",col="green")
text(0,-.28, substitute(paste("mean ",mu," and standard deviation ",sigma)), cex=1.5)
```

Interpretation of standard deviation:

* $\approx$ 68\% of the population is within 1 standard deviation of the mean.
* $\approx$ 95\% of the population is within 2 standard deviations of the mean.
* $\approx$ 99.7\% of the population is within 3 standard deviations of the mean.

----

### Normal $z$-score
* If $Y\sim \texttt{norm}(\mu,\sigma)$ then the corresponding $z$-score is
  $$Z=\frac{Y-\mu}{\sigma}=\frac{\mathtt{observation-mean}}{\mathtt{standard\
      deviation}}$$
*  I.e. $Z$ counts the number of standard deviations that the
  observation lies away from the mean, where a negative value tells
  that we are below the mean.
* We have that $Z\sim \texttt{norm}(0,1)$, i.e. $Z$ has zero mean
   and standard deviation one.
* This implies that
    * $Z$ lies between $-1$ and $1$ with probability 68\%
    * $Z$ lies between $-2$ and $2$ with probability 95\%
    * $Z$ lies between $-3$ and $3$ with probability 99.7\%
* It also implies that:
    * The probability of $Y$ being between $\mu - z\sigma$ and $\mu + z\sigma$
      is equal to the probability of $Z$ being between $-z$ and $z$.

----

### Calculating probabilities in the standard normal distribution

* The function `pdist` always outputs the area to the left of the $z$-value (quantile/percentile) we give as input (variable `q` in the function), i.e. it outputs the probability of getting a value less than $z$. The first argument of `pdist` denotes the distribution we are considering.

```{r}
# For a standard normal distribution the probability of getting a value less than 1 is:
left_prob <- pdist("norm", q = 1, mean = 0, sd = 1)
left_prob
```

* Here there is a conflict between **R** and the textbook, since in the book we always consider right probabilities in the normal distribution. Since the total area is 1 and we have the left probability we easily get the right probability:

```{r}
right_prob <- 1 - left_prob
right_prob
```

* For $z=1$ we have a right probability of $p=0.1587$, so the probability of an observation between $-1$ and $1$ is $1 - 2 \cdot 0.1587 = 0.6826 = 68.26\%$ due to symmetry.

----

### Calculating $z$-values (quantiles) in the standard normal distribution

* If we have a probability and want to find the corresponding $z$-value we again need to decide on left/right probability. The default in **R** is to find the left probability, so if we want the $z$-value with e.g. 0.5% probability to the left we get:
```{r}
left_z <- qdist("norm", p = 0.005, mean = 0, sd = 1, xlim = c(-4, 4))
left_z
```

* However, in all the formulas in the course we follow the textbook and consider $z$-values for a given right probability.\ E.g. with 0.5% probability to the right we get:
```{r}
right_z <- qdist("norm", p = 1-0.005, mean = 0, sd = 1, xlim = c(-4, 4))
right_z
```

* Thus, the probability of an observation between $-2.576$ and $2.576$ equals $1 - 2 \cdot 0.005 = 99\%$.

----

### Example
  The Stanford-Binet Intelligence Scale is calibrated to be approximately normal
  with mean 100 and standard deviation 16.

  What is the 99-percentile of IQ scores?

* The corresponding $z$-score is $Z=\frac{IQ-100}{16}$, which
    means that $IQ=16Z+100$.
* The 99-percentile of $z$-scores has the value 2.326 (can be calculated using `qdist`).
* Then, the 99-percentile of IQ scores is:
  $$
  IQ=16\cdot 2.326+100=137.2.
  $$
* So we expect that one out of hundred has an IQ exceeding 137.


# Distribution of sample statistic

## Estimates and their variability

  We are given a sample $y_1,y_2,\ldots,y_n$.

* The sample mean $\bar{y}$ is the most common
  estimate of the population mean $\mu$.
* The sample standard deviation, $s$, is the most common
  estimate of the population standard deviation $\sigma$.

  We notice that there is an uncertainty (from sample to sample) connected to
  these statistics and therefore we are interested in describing their **distribution**.


## Distribution of sample mean
* We are given a sample $y_1,y_2,\ldots,y_n$ from a population with
  mean $\mu$ and standard deviation $\sigma$.
* The sample mean $$\bar{y}=\frac{1}{n}(y_1+y_2+\ldots+y_n)$$ then has
  a distribution where
    + the distribution has mean $\mu$,
    + the distribution has standard deviation
        $\frac{\sigma}{\sqrt{n}}$ (also called
        the **standard error**), and
    + when $n$ grows, the distribution approaches a normal
        distribution. This result is called **the central limit theorem**.

----

### Central limit theorem
*  The points above can be summarized as $${\bar{y}}\approx
  \texttt{norm}(\mu,\frac{\sigma}{\sqrt{n}})$$ i.e. $\bar{y}$ is approximately
  normally distributed with mean $\mu$ and standard error
  $\frac{\sigma}{\sqrt{n}}$.
* When our sample is sufficiently large (such that the above approximation is good)
  this allows us to make the following observations:
    * We are $95\%$ certain that $\bar{y}$ lies in the interval from
        $\mu-2\frac{\sigma}{\sqrt{n}}$ to $\mu+2\frac{\sigma}{\sqrt{n}}$.
    * We are almost completely certain that $\bar{y}$ lies in the
        interval from $\mu-3\frac{\sigma}{\sqrt{n}}$ to $\mu+3\frac{\sigma}{\sqrt{n}}$.
* This is not useful when $\mu$ is unknown, but let us rephrase the
  first statement to:
    * We are $95\%$ certain that $\mu$ lies in the interval from
    $\bar{y}-2\frac{\sigma}{\sqrt{n}}$ to
    $\bar{y}+2\frac{\sigma}{\sqrt{n}}$, i.e. we are directly talking
    about the uncertainty of determining $\mu$.

----

### Illustration of CLT
   The central limit theorem is illustrated in Agresti:

![Central limit theorem illustrated in Agresti](https://asta.sfx.aau.dk/static-files/asta/img/AgrestiCLT.png)

----

### Example

* Body Mass Index (BMI) of people in Northern Jutland (2010) has mean
  $\mu=25.8$ kg/$\mathtt{m}^2$ and standard deviation $4.8$ kg/$\mathtt{m}^2$.
* A random sample of $n=100$ costumers at a burger bar had an average BMI
  given by $\bar{y}=27.2$.
* If "burger bar" has "no influence" on BMI (and the sample
  is representative of the population/people in Northern Jutland), then
  $$\bar{y} \approx \texttt{norm}(\mu,\frac{\sigma}{\sqrt{n}})=\texttt{norm}(25.8,0.48).$$
* For the actual sample this gives the observed $z$-score
  $$z_{obs}=\frac{27.2-25.8}{0.48}=2.92$$
* Recalling that the $z$-score is (here approximately) standard normal,
  the probability of getting a higher $z$-score is:

```{r}
1 - pdist("norm", mean = 0, sd = 1, q = 2.92, xlim = c(-4, 4))
```

* Thus, it is highly unlikely to get a random sample with such a high
  $z$-score. This indicates that costumers at the burger bar has a mean
  BMI, which is higher than the population mean.



```{r, include = FALSE}
# Waiting times
# set.seed(1)
# y_canteen_tmp <- y_canteen[y_canteen < 20]
# y <- cbind(y_canteen_tmp, replicate(4, sample(y_canteen_tmp, replace = TRUE)))
# hist(apply(y, 1, mean), breaks = "FD")
```

