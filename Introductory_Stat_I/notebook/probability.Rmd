---
title: "Probabilidade"
author: "Fernando B. Sabino da Silva"
output:
  pdf_document:
    fig_caption: no
    highlight: tango
    keep_tex: yes
    number_section: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
  slidy_presentation:
    css: https://asta.math.aau.dk/course/asta/2018-1/?file=lecture_style.css
    fig_caption: no
    highlight: tango
    theme: cerulean
  ioslides_presentation:
    highlight: tango
---

```{r, include = FALSE}
## Remember to add all packages used in the code below!
missing_pkgs <- setdiff(c("mosaic", "pander", "VennDiagram"), rownames(installed.packages()))
if(length(missing_pkgs)>0) install.packages(missing_pkgs)
library(VennDiagram)
```

# Probabilidade de eventos

## O conceito de probabilidade

* Experimento: Medir o tempo de espera em uma fila. Se o tempo exceder 2 minutos marque $1$ e 0, caso contrário.
* O experimento é realizado $n$ vezes com resultados $y_1,y_2,\ldots,y_n$. Há uma **variação aleatória** no resultado, i.e. às vezes ocorre $1$ e em outras vezes ocorre 0.
* **Probabilidade empírica** de exceder 2 minutos:\
    $$ 
    p_n = \frac{\sum_{i=1}^n y_i}{n}.
    $$
* **Probabilidade teórica** de exceder 2 minutos:\ 
    $$
    p = \lim_{n\rightarrow\infty} p_n.
    $$.
* Tentamos inferir o verdadeiro valor de $p$ com base em uma amostra.
  Por exemplo, "$p > 0.1$?" 
  ("mais de $1$0\% dos clientes experimentaram um tempo de espera superior a 2 minutos?").
* A (inferência) estatística está preocupada com tais questões que são úteis para a tomada de decisões. Em geral, só temos acesso a uma amostra finita.



## Experimento real

* Em um determinado mês de 2017,\ um grupo de estudantes respondeu a pergunta de quanto tempo eles precisaram esperar na fila em uma determinada cantina (em minutos):
```{r}
y_cantina <- c(2, 5, 1, 6, 1, 1, 1, 1, 3, 4, 1, 2, 1, 2, 2, 2, 4, 2, 2, 5, 20, 2, 1, 1, 1, 1)
x_cantina <- ifelse(y_cantina > 2, 1, 0)
x_cantina
```
* Probabilidade empírica de esperar mais de 2 minutos:
```{r}
p_cantina <- sum(x_cantina) / length(x_cantina)
p_cantina
```
* Questão: A probabilidade na população é  $p > 1/3$?
* Nota: Um estudante disse que esperou por 20 minutos. Dado os outros resultados, podemos duvidar da veracidade e ignorar essa observação).

```{r, include = FALSE}
# set.seed(1)
#y<-y_canteen
# y_ref <- replicate(1000, sample(y, replace = TRUE))
# p_ref <- apply(y_ref, 2, function(x) mean(x > 2))
# quantile(p_ref, c(0.025, 0.975))
# 
# f <- function(x) quantile(p_ref, x, names = FALSE) - 1/3
# 1 - uniroot(f, c(0, 1))$root
# mean(p_ref > 1/3)
```



## Outro experimento

* John Kerrich, um matemático sul-africano, estava visitando Copenhague quando a segunda Guerra Mundial eclodiu. Dois dias antes ele estava programado para viajar para a Inglaterra quando os alemães invadiram a Dinamarca. Kerrich passou o resto da guerra em um acampamento e para passar o tempo realizou uma série de experimentos. Em um destes, ele jogou uma moeda 10,000 vezes. Os resultados são mostrados a seguir.
* Abaixo, `x` é um vetor com os primeiros 2000 resultados do experimento de John Kerrich.
(0 = coroa,\ $1$ = cara):
```{r, include = FALSE}
x <- c(0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 
0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 
0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 
1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 
0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 
1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 
0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 
1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 
1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 
0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 
1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 
1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 
1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 
0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 
0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 
1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 
1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 
0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 
0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 
1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 
1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 
1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 
1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 
0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 
1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 
0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 
0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 
1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 
1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 
0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 
0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 
1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 
1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 
0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 
0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 
0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 
1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 
1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 
0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 
0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 
1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 
1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 
0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 
0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 
0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 
0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 
1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 
0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 
1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 
0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 
0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 
0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 
0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 
0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 
0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 
0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 
1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 
1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 
0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 
0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 
1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 
0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 
1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 
0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 
0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 
1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 
0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 
1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 
0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 
1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 
0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 
1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 
1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 
0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 
1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 
0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 
0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 
0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 
1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 
0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 
0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 
1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 
1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 
0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 
0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 
0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 
1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 
0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 
0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 
1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 
1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 
0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 
1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 
1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 
1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 
0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 
1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 
1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 
1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 
0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 
1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 
1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 
1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 
0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 
1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 
0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 
1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 
0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 
1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 
0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 
0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 
1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 
1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 
0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 
0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 
1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 
0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 
0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 
1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 
1)
```
```{r}
head(x, 10)
```
* Gráfico da probabilidade empírica $p_n$ de sair cara contra o número de lançamentos $n$:

```{r, echo=FALSE, fig.height = 5, fig.width = 6, fig.align = "center"}
plot(cumsum(x)/seq_along(x), type = "l", log = "x", xlab = "n", ylab = expression(p[n]))
abline(h = 0.5, lty = 2)
```

(O eixo horizontal está na escala log).



## Definições

* **Espaço Amostral**:\ Todos os possíveis resultados de um experimento.
* **Evento**:\ Qualquer subconjunto do espaço amostral.

Nós conduzimos o experimento $n$ vezes. Seja $\#(A)$ o número de vezes que observamos o evento $A$.

* **Probabilidade empírica** do evento $A$:
  $$
  p_n(A)=\frac{\#(A)}{n}.
  $$
* **Probabilidade teórica** do evento $A$:
  $$
  P(A)=\lim_{n\to\infty}p_n(A)
  $$
<!-- * Nota: $0\leq P(A)\leq 1$. -->



## Probabilidades teóricas de dois eventos

* Se dois eventos $A$ e $B$ são **disjuntos** (sem intersecção) então  
    + $\#(A \text{ e } B) = 0$ implica que $P(A \cap B)=0.$
    + $\#(A \text{ ou } B)=\#(A)+\#(B)$ implica que $P(A \cup B)=P(A)+P(B).$
  
```{r, echo = FALSE, fig.height = 3.5, fig.width = 4.5}
venn.plot <- draw.pairwise.venn(area1 = 70, area2 = 70, cross.area = 0, 
                                category = c("A", "B"), cex = 0, cat.cex = 4)
grid.draw(venn.plot)
grid.newpage()
```

* Se dois eventos $A$ e $B$ **não são disjuntos** então a fórmula mais geral é
  $$
  P(A \cup B) = P(A) + P(B) - P(A \cap B).
  $$

```{r, echo = FALSE, fig.height = 3.5, fig.width = 4.5}
venn.plot <- draw.pairwise.venn(area1 = 70, area2 = 70, cross.area = 30, 
                                category = c("A", "B"), cex = 0, cat.cex = 4)
grid.draw(venn.plot)
grid.newpage()
```

## Probabilidade Condicional

* Digamos que consideramos dois eventos $A$ e $B$.\ Então a **probabilidade condicional** de $A$ dado 
(ou condicional ao) o evento $B$ é escrito $P(A \mid B)$ e é definido por
$$P(A \mid B)=\frac{P(A \cap B)}{P(B)}.$$
* A probabilidade acima pode ser entendida do seguinte modo: "quão provável é o evento $A$ se nós sabemos que $B$ ocorreu". 

---- 

### Exemplo com os dados das revistas:
```{r message=FALSE}
magAds <- read.delim("C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/datasets_ads.txt")

# Criando dois novos fatores: 'words' e 'education':
magAds$words <- cut(magAds$WDS, breaks = c(31, 72, 146, 230), include.lowest = TRUE)
magAds$education <- factor(magAds$GROUP, levels = c(1, 2, 3), labels = c("high", "medium", "low"))

library(mosaic)
tab <- tally( ~ words + education, data = magAds)
tab
```

* O evento $A$=$\{$words=`r levels(magAds$words)[3]`$\}$ (o anúncio é um texto "difícil") tem probabilidade empírica
$$ p_n(A) = \frac{9 + 6 + 5}{54} = \frac{20}{54} \approx 37 \%.$$
* Digamos que só estamos interessados na probabilidade de um texto "difícil" (evento $A$) para revistas de educação superior (high education), i.e. condicionando no evento         $B=\{$education=high$\}$. Então a probabilidade condicional empírica pode ser calculada a partir da tabela acima:
  
  $$
  p_n(A \mid B) = \frac{9}{4+5+9} = \frac{9}{18} = 0.5 = 50\%.
  $$
  
* A probabilidade condicional de $A$ dado $B$ pode ser (teoricamente) expressada por

  $$
  \begin{aligned}
  P(A \mid B) 
  &= P(\text{words} =`r levels(magAds$words)[3]` \mid \text{education = high})  \\[0.5em]
  &= \frac{P(\text{words} =`r levels(magAds$words)[3]` \cap \text{education = high})}{P(\text{education = high})},  \\
  \end{aligned}
  $$
  que traduzindo para a probabilidade empírica (substituindo $P$ por $p_n$) dará

  $$
  \begin{aligned}
  p_n(A \mid B) 
  &= \frac{p_n(\text{words} =`r levels(magAds$words)[3]` \cap \text{education = high})}{p_n(\text{education = high})}  \\
  &= \frac{\frac{9}{54}}{\frac{4+5+9}{54}} \\
  &= \frac{9}{4+5+9} \\[0.5em]
  &= 50\%
  \end{aligned}
  $$
  como calculado acima.

## Probabilidade condicional e independência
* Se a informação sobre $B$ não muda a probabilidade de $A$ nós dizemos que $A$ é **independente** de $B$ e escrevemos
  $$
  P(A \mid B) = P(A) \quad \Leftrightarrow  \quad P(A \cap B) = P(A)P(B)
  $$
  Quando isto acontecer nós dizemos que
  $A$ e $B$ são **eventos independentes**.
* Em geral, os eventos $A_1, A_2, ..., A_k$ são independentes se
  $$
  P(A_1 \cap A_2 \cap ... \cap A_k) = P(A_1) P(A_2) \cdots P(A_k)
  $$
  e se o produto de todas as combinações de ordem 2 a k-1 também são válidas, i.e., k eventos são ditos independentes se são **independentes k a k, (k-1) a (k-1),..., dois a dois**, isto é, para cada subconjunto $S$ de ${1, 2,\ldots k}$.
  
### Dados das revistas revisitados
* Lembre-se das probabilidades empíricas calculadas acima:
$$
p_n(A) = 37 \%
\quad \text{e} \quad
p_n(A \mid B) = 50\%.
$$
* Isto indica (não podemos dizer com certeza, pois só temos a disposição uma amostra finita - no curso de estatística II vemos detalhes de como testar isso) que a probabilidade teórica

$$
P(A) \neq P(A \mid B)
$$
e, portanto, que o conhecimento sobre o evento $B$ (nível de educação elevado) pode transmitir informações sobre a probabilidade do evento $A$ (o anúncio contém um texto "difícil").

## Um pouco mais de formalidade
  
### Modelos de Probabilidade
Quando discutimos modelos de probabilidade, nós falamos de **experimentos** aleatórios que produzem um dos vários **resultados** possíveis. Um **modelo de probabilidade** que descreve a incerteza de um experimento consiste em três elementos (descrevemos dois abaixo):

* O **espaço amostral**, geralmente denominado por $\Omega$, representando o conjunto que contém todos os resultados possíveis.
* Uma **função de probabilidade** que atribui a um evento $A$ um número não-negativo, $P[A]$, que representa a probabilidade de que o evento $A$ ocorra como resultado do experimento.

Nós chamamos de $P[A]$ a **probabilidade** do evento $A$. Um evento $A$ pode ser qualquer subconjunto do espaço amostral, não necessariamente um único resultado possível. As leis da probabilidade devem seguir uma série de regras, que são o resultado de um conjunto de axiomas que introduziremos agora.

## Axiomas de Probabilidade

Dado um espaço amostral $\Omega$ para um experimento em particular, a **função de probabilidade** associada ao experimento deve satisfazer os seguintes axiomas.

  + *Não-negatividade*: $P[A] \geq 0$ para qualquer evento $A \subset\Omega$.
  + *Normalização*: $P[\Omega] = 1$. Ou seja, a probabilidade de todo o espaço é $1$.
  + *Aditividade*: Para eventos mutuamente exclusivos $E_1, E_2,\ldots$ $$P\left[\bigcup_{i = 1}^{\infty} E_i\right] = \sum_{i = 1}^{\infty} P[E_i]$$.
  
  Usando estes axiomas, muitas regras adicionais de probabilidade podem ser facilmente derivadas.
  

## Regras de Probabilidade

* Dado um evento $A$ e seu complemento, $A^c$, ou seja, os resultados em $\Omega$ que não estão em $A$, temos a regra do **complementar**:

$$P[A^c] = 1 - P [A]$$

* Em geral, para dois eventos $A$ e $B$, temos a **regra da adição**:

$$P[A \cup B] = P[A] + P[B] - P[A \cap B]$$
*Exercício: Prove que as relações acima são válidas usando os três axiomas de probabilidade. Prove também que se* $A\subseteq B$, *então* $P(A)\leq P(B)$. 
    
* Se $A$ e $B$ também são *disjuntos*, então temos:

$$ P[A \cup B] = P[A] + P[B] $$
Se tivermos $n$ eventos mutuamente exclusivos, $E_1, E_2,\ldots E_n$, então temos:

$$ P\left[\textstyle \bigcup_{i = 1}^{n} E_i \right] = \sum_{i = 1}^{n} P[E_i] $$

## Regra de Bayes

Defina uma **partição** de um espaço amostral $\Omega$ como um conjunto de eventos disjuntos $A_1, A_2, \ldots, A_n$ cuja união é o espaço amostral $\Omega$. Isso é

$$A_i \cap A_j = \emptyset $$

para todos $i \neq j$ e

$$ \bigcup_{i = 1}^{n} A_i = \Omega. $$

Seja $A_1, A_2, \ldots, A_n$ formam uma partição do espaço amostral, onde $P[A_i]>0$ para todo $i$. Então para qualquer evento $B$ com $P[B]>0$ temos a **Regra de Bayes**:

$$ P[A_i | B] = \frac{P[A_i] P[B | A_i]} {P[B]} = \frac{P[A_i] P[B | A_i]}{\sum_{i = 1}^{n} P[A_i] P[B | A_i]} $$

O denominador da última igualdade é freqüentemente chamado de **lei da probabilidade total**:

$$ P[B] = \sum_{i = 1}^{n} P[A_i] P[B | A_i] $$

Dois eventos $A$ e $B$ são considerados **independentes** se satisfizerem

$$ P[A \cap B] = P[A] \cdot P[B] $$

Uma coleção de eventos $E_1, E_2, \ldots E_n$ é considerada independente se

$$ P\left[\bigcap_{i\in S} E_i \right] = \prod_{i \in S} P[E_i] $$

para cada subconjunto $S$ de ${1, 2,\ldots n}$.

## Regra de Bayes (Continuação)
  
* As probabilidades dos estados da natureza são alteradas de acordo com as informações obtidas. Sempre que coletarmos uma amostra e ela contiver informações relevantes, as nossas probabilidades serão revisadas. Quanto maior informação, menor a incerteza.

* Não sabemos qual é o **verdadeiro** estado da natureza (o que realmente ocorrerrá), mas temos uma avaliação das probabilidades (chamadas de probabilidades a priori, digamos, $P(S_j)$).

* Queremos calcular as probabilidades revisadas, digamos,  $P(S_j | E_i)$, mas sabemos as **verossimilhanças** (likelihood, em inglês) $P(E_i | S_j)$ = probabilidade do resultado experimental ($E_i$) condicional aos estados de natureza ($S_j$).

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/Bayes.jpg)



## Exercício: Problema de Diagnóstico Médico  

$1$. Diagnóstico Médico: Suponha que $1$% da população de um determinado lugar seja HIV positivo. O Departamento de Saúde Pública tem um teste de diagnóstico barato que é administrado às pessoas que pedem para serem testadas. O teste resulta em uma indicação positiva ou negativa e apresenta as seguintes características:

(a) O teste dá uma indicação positiva (verdadeira) 99% das vezes em que uma pessoa é realmente portadora do vírus. Infelizmente, o teste dá uma indicação negativa (falsa) para $1$% das pessoas que portam o vírus.

(b) O teste dá uma indicação negativa (verdadeira) 95% das vezes em que uma pessoa não é portadora do vírus (e, portanto, dá uma indicação positiva (falsa) para 5% das pessoas). 

* Suponha que um residente escolhido ao acaso tenha acabado de fazer o teste. Para seu choque, o teste resultou em uma indicação positiva. Qual é a probabilidade dele ser realmente portador do vírus?

* Suponha agora que o teste tenha resultado em uma indicação negativa. O residente deve ficar confiante de que ele não é realmente HIV positivo?

* Como as suas respostas mudam se o teste se tornar $10$ vezes mais confiável, isto é, se as taxas de erro para falsos negativos e falsos positivos, respectivamente, forem reduzidas para $0.1$% e 0.5%?



## Exercício: Problema de Controle de Qualidade

$2$. Controle de Qualidade: A DataSafe produz pen drives. Seu processo de fabricação é direcionado para produzir defeitos a uma taxa de $1/10$ de $1$%, isto é, $0.1$%. Às vezes, quando o processo é iniciado, ele desliza ligeiramente e produz defeitos a uma taxa de $2/10$ de $1$%. Como parar o maquinário para recalibrar o processo é muito caro, a DataSafe está disposta a operar com uma taxa de defeitos um pouco maior. Ocasionalmente, no entanto, o processo se desvia tanto que produz defeitos a uma taxa de $5/10$ de $1$%, o que é inaceitável para a gerência. Com base em experiências passadas, o engenheiro chefe da DataSafe, observou que, em qualquer dia, as probabilidades para a verdadeira taxa de defeitos são:

  Condição Operacional    Boa      Ok     Ruim  
  ---------------------  ------  ------  ------
  Taxa de Defeito        $.001$  $.002$  $.005$
  Probabilidade          $.75$   $.24$   $.01$
  
* No início de cada dia, ele retira uma amostra da produção inicial para deciri se a máquina precisa ser recalibrada. A sua regra operacional é recalibrar sempre que ele não puder ter pelo menos 90% de certeza de que a taxa de defeitos está abaixo de $5/10$ de $1$%.
  
* Suponha que o primeiro pen drive escolhido aleatoriamente esteja com defeito. A produção deveria ser parada para recalibração? E se os dois primeiros escolhidos aleatoriamente estiverem com defeito?
  

## Solução para o problema do diagnóstico médico 

Dada uma indicação positiva: 

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/hiv+.jpg)

Dada uma indicação negativa:

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/hiv-.jpg)

* Se o teste se tornar $10$ vezes mais confiável.

Dada uma indicação positiva:

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/hiv+10.jpg)


Dada uma indicação negativa:

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/hiv-10.jpg)



## Solução para o Problema de Controle de Qualidade

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/QC1.jpg)
![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/QC2.jpg)

* Probabilidades revisadas dado que um pen drive defeituoso é selecionado:

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/revised.jpg)

* Então, P(Bom ou OK | defeituoso) = 0.9609 > 0.9. Ele não deve recalibrar.

* Note as implicações disso. Se o pen drive escolhido for defeituoso, ele não vai recalibrar. E se não estiver com defeito, ele certamente não irá recalibrar. Logo, amostrar apenas um pen drive é sem utilidade. 

* Probabilidades revisadas dado que dois pen drives defeituosos são selecionados: 

![](C:/Users/fsabino/Desktop/Codes/papers/Introductory_Stat_I/notebook/revised2.jpg)

* Então, P(Bom ou OK | 2 de 2 defeituosos) = 0.87245 < 0.9. Ele deve recalibrar.

* Deixo para você verificar que ele não irá recalibrar se nenhum pen drive amostrado estiver com defeito (seja calculando ou inferindo pelos que encontramos na parte anterior) ou se apenas um dos dois estiver com defeito.


## Distribuição discreta
### Exemplo: Dados das revistas
```{r size="small",tidy=FALSE,message=FALSE}
# Tabela contendo o percentual de anúncios em cada combinação dos níveis de 'words' e 'education'
tab <- tally( ~ words + education, data = magAds, format = "percent")
round(tab, 2) # Duas casas decimais
```

* Os 9 eventos disjuntos acima (correspondentes as combinações de `words` e
  `education`) compõem todo o espaço amostral para as duas variáveis. As probabilidades empíricas de cada evento são dadas na tabela.

### Distribuição discreta
* Em geral:
    + Seja $A_1,A_2,\ldots,A_k$ uma subdivisão do espaço amostral em 
    eventos disjuntos (par a par).
    + As probabilidades $P(A_1), P(A_2), ..., P(A_k)$ (**distribuição discreta**) satisfazem
    $$\sum_{i=1}^kP(A_i)=1.$$
    
----


### Exemplo: Três lançamentos de uma moeda 

* **Variável aleatória**: Uma variável aleatória é simplesmente uma função $Y$ que mapeia os resultados do espaço amostral para números reais, isto é, que mapeia os possíveis resultados do experimento em um número.
* Resultados possíveis de um experimento com 3 lançamentos de moedas:
    + $0$ caras (KKK)
    + $1$ cara (CKK, KCK, KKC)
    + $2$ caras (CCK, CKC, KCC)
    + $3$ caras (CCC)
* Os eventos acima são disjuntos e compõem todo o espaço amostral.
* Seja $Y$ o número de caras no experimento: $Y(KKK) = 0, Y(CKK) = 1, \ldots$
* Assuma que cada resultado é igualmente provável, i.e. $1/8$ de probabilidade para cada um deles.
  Então,
    + $P(\text{nenhuma cara}) = P(Y = 0) = P(KKK) = 1/8.$
    + $P(\text{uma cara}) = P(Y = 1) = P(CKK \text{ ou } KCK \text{ ou } KKC) = P(CKK) + P(KCK) + P(KKC) = 3/8.$
    + Similarmente para 2 ou 3 caras.
* Então, a distribuição de $Y$ é

  Número de caras, $Y$   $0$  $1$ $2$ $3$
  --------------------- ----  --- --- ----
  Probabilidade         $1/8$ 3/8 3/8 $1/8$



# Distribuição de variáveis aleatórias

## Distribuição de probabilidade
* Exemplo: Conduzimos um experimento no qual fazemos uma medição quantitativa $Y$ (uma variável aleatória), por exemplo, contamos o número de palavras em um anúncio ou o tempo de espera em uma fila.
* De antemão, há muitos resultados possíveis para os experimentos, i.e. os valores de $Y$ que irão acontecer em uma realização do experimento são incertos, mas nós podemos quantificá-los pela **distribuição de probabilidade** de $Y$.

* Uma definição não rigorosa, mas útil para transmitir a ideia é:
$$ \text{distribuição} = \text{lista de possíveis} \textbf{ valores} + \text{probabilidades} \textbf{ associadas} $$

* Para qualquer intervalo $(a, b$), a distribuição indica a probabilidade de observar um valor da variável aleatória $Y$ neste intervalo:
  $$ P(a<Y<b),\qquad -\infty < a < b < \infty.$$
* Se os possíveis valores de uma variável aleatória são discretos, isto é, se nós podemos enumerar todos os possíveis valores de $Y$, então a variável aleatória $Y$ é chamada de **discreta**. Por exemplo, o número de palavras em um anúncio. 
* Se os possíveis valores de uma variável aleatória são contínuos, isto é, $Y$ pode assumir qualquer valor dentro de um intervalo, então a variável aleatória $Y$ é chamada de **contínua**. Por exemplo, o tempo de espera em uma fila.

## Variáveis Aleatórias Discretas
* A distribuição de uma variável aleatória discreta $X$ é mais frequentemente especificada por uma lista de possíveis valores e uma função massa de probabilidade $p(x)$, isto é, 

$$ p(x) = p_X(x) = P[X = x]. $$
Costumeiramente nós abandonamos o subscrito da notação mais correta $p_X(x)$ e simplesmente escrevemos $p(x)$. A variável aleatória relevante será discernida do contexto.

O exemplo mais comum de uma variável aleatória discreta é a variável aleatória com distribuição binomial. A função massa de uma variável aleatória $X$ com distribuição binomial é dada por

$$ P(X = x | n, p) = p_X(x | n, p) = {n \choose x} p^x(1 - p)^{n - x}, \ \ \ x = 0, 1, \ldots, n, \ n \in \mathbb{N}, \ 0 < p < 1. $$

A última linha contém uma grande quantidade de informação.

* A função $p_X(x | n, p)$ é a função massa. É uma função de $x$, os possíveis valores da variável aleatória $X$. Ela é condicional aos parâmetros $n$ e $p$. Valores diferentes destes parâmetros especificam distribuições binomiais diferentes.

* $x = 0, 1, \ldots, n$ indica o espaço amostral de $x$, isto é, os possíveis valores da variável aleatória.

$n \in \mathbb{N}$ e $0 < p < 1$ especificam os espaços dos parâmetros. Estes são os valores possíveis dos parâmetros que fornecem uma distribuição binomial válida. Frequentemente, toda essa informação é simplesmente codificada escrevendo

$$ X \sim \text{Bin}(n, p). $$

## Variáveis Aleatórias Contínuas
* A distribuição de uma variável aleatória contínua $X$ é mais frequentemente especificada pelo conjunto de possíveis valores e uma função densidade de probabilidade, $f(x)$. (A função de distribuição acumulada (densidade acumulada), $F(x)$ ou a função característica ou geratriz de momentos também seriam suficientes.)

* A probabilidade do evento $a < X < b$ é calculada por

$$ P[a < X < b] = \int_{a}^{b} f(x)dx. $$

Note que as densidades não são probabilidades.

* O exemplo mais comum de uma variável aleatória contínua é uma variável aleatória com distribuição normal. A densidade de uma variável aleatória normal $X$, é dada por

$$ f(x | \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2p}} \cdot \exp\left[\frac{-1}{2} \left(\frac{x - \mu}{\sigma}\right)^2 \right], \ \ \ -\infty < x < \infty, \ -\infty < \mu < \infty, \ \sigma > 0. $$

* A função $f(x | \mu, \sigma^2)$ é a função de densidade. É uma função de  $x$, os valores possíveis da variável aleatória $X$. Ela é condicional aos parâmetros $\mu$ e $\sigma^2$. Diferentes valores destes parâmetros especificam diferentes distribuições normal.
* $-\infty < x < \infty$ indica o espaço amostral de $x$. Neste caso, a variável aleatória pode assumir todo e qualquer valor real.
* $-\infty < \mu < \infty$ and $\sigma > 0$ especificam o espaço de parâmetros. Estes são os valores possíveis dos parâmetros que fornecem uma distriuição normal válida. Muitas vezes, toda essa informação é simplesmente codificada escrevendo

$$ X \sim N(\mu, \sigma^2) $$

## Independência entre Variáveis Aleatórias
Considere duas variáveis aleatórias $X$ e $Y$. Nós dizemos que elas são independentes se

$$ f(x, y) = f(x) \cdot f(y) $$

para todo $x$ e $y$. Aqui $f(x, y)$ é a função densidade (massa) conjunta de $X$ e $Y$. Nós chamamos de $f(x)$ e de $f(y)$ as funções densidade (massa) marginais de $X$ e de $Y$, respectivamente. 

* A função densidade (massa) conjunta $f(x, y)$ juntamente com os possíveis valores $(x, y)$ especificam a distribuição conjunta de $X$ e $Y$.

Noções similares existem para mais de duas variáveis aleatórias.
    
### Amostra Aleatória

  Nós realizamos um experimento $n$ vezes, onde o resultado do $i$-ésimo
  experimento corresponde a uma medição de uma variável aleatória $Y_i$,
  onde assumimos que

* Os experimentos são **independentes**
* As variáveis $Y_1,\ldots,Y_n$ têm a **mesma distribuição**

## Parâmetros populacionais

* Quando o tamanho da amostra aumenta, a média da amostra, $\overline{y}$, por exemplo, irá se estabilizar em torno de um valor fixo, $\mu$, que é usualmente desconhecido. O valor $\mu$ é chamado de **média populacional**.
* Correspondentemente, o desvio padrão da amostra, $s$, irá se estabilizar em torno de um valor fixo, $\sigma$, que geralmente é desconhecido. O valor $\sigma$ é chamado de  **desvio padrão da população**.
* Notação:
	+ $\mu$ (mu) denota a médida da população.
	+ $\sigma$ (sigma) denota o desvio padrão da população.

| População  | Amostra      |
|:----------:|:------------:|
|$\mu$       |$\overline{y}$|
|$\sigma$    |$s$           |

----

### Distribuição de uma variável aleatória discreta

* Valores possíveis para $Y$:\ $\{y_1,y_2,\ldots,y_k\}$.
* A **distribuição** de $Y$ é a probabilidade de cada valor possível:\ $p_i=P(Y=y_i), \quad i=1,2,\ldots,k$.
* A distribuição satisfaz: $\sum_{i=1}^kp_i=1$.

## Esperança
Para variáveis aleatórias discretas, nós definimos a **esperança** da função de uma variável aleatória $X$ da seguinte maneira.

$$ \mathbb{E}[g(X)] \triangleq \sum_{x} g(x)p(x) $$

Para variáveis aleatórias contínuas, nós temos uma definição semelhante.

$$ \mathbb{E}[g(X)] \triangleq \int g(x)f(x) dx $$

Para funções específicas $g$, as esperanças recebem nomes

A **média** de uma variável aleatória $X$ é dada por

$$ \mu_{X} = \mathbb{E}[X]. $$

Então, para uma variável aleatória discreta, nós temos 

$$ \text{E}[X] = \sum_{x} x \cdot p(x) $$

Para uma variável aleatória contínua, simplesmente substituímos a soma por uma integral.

A variância de uma variável aleatória $X$ é dada por

$$ \sigma^2_{X} = \text{var}[X] \triangleq \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2. $$

O desvio padrão de uma variável aleatória $X$ é dada por

$$ \sigma_{X} = \text{sd}[X] \triangleq \sqrt{\sigma^2_{X}} = \sqrt{\text{var}[X]}. $$

A covariância de variáveis aleatórias $X$ e $Y$ é dada por

$$ \text{cov}[X, Y] \triangleq \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X] \cdot \mathbb{E}[Y]. $$

## Verossimilhança (Likelihood)

Considere $n$ variáveis aleatórias iid $X_1, X_2, \ldots X_n$. Nós definimos a função de verossimilhança por

$$ \mathcal{L}(\theta \mid x_1, x_2, \ldots x_n) = \prod_{i = i}^n f(x_i; \theta) $$

onde $f(x_i; \theta)$ é a função densidade (ou massa) da variável aleatória $X_i$ avaliada em $x_i$ com parâmetro $\theta$.

Enquanto a probabilidade é uma função de um possível valor (ou intervalo) observado, dados determinados valores dos parâmetros, a verossimilhança é o "oposto": é uma função dos valores possíveis dos parâmetros dada a amostra, i.e., verossimilhança é uma medida da evidência que uma amostra fornece para valores específicos dos parâmetros em um modelo paramétrico.

A maximização da verossimilhança é uma técnica comum para ajustar um modelo aos dados.

## Valor esperado (média) para uma distribuição discreta

* O **valor esperado** ou **média (populacional)** de $Y$ é
  $$
      \mu = \sum_{i=1}^k p_iy_i
  $$
* Uma propriedade importante do valor esperado é que ele tem a mesma unidade de medida das observações (por exemplo, metro).

### Exemplo: número de caras em 3 lançamentos de uma moeda
* Lembre-se da distribuição de $Y$ (número de caras):

      | y (número de caras) | 0  | $1$ | 2 | 3 |
      |:--------------------|:---:|:-:|:-:|:-:|
      |     $P(Y = y)$      |$1/8$|3/8|3/8|$1/8$|

* Então o valor esperado é

  $$
  \mu = 0\frac{1}{8}+1\frac{3}{8}+2\frac{3}{8}+3\frac{1}{8}=1.5.
  $$

  *Observe que o valor esperado não precisa ser um resultado possível do próprio experimento.*


## Variância e desvio padrão de uma distribuição discreta

* A **variância (populacional)** de $Y$ é
    $$
        \sigma^2 = \sum_{i=1}^k (y_i - \mu)^2 p_i
    $$
* O **desvio padrão (populacional)** é $\sigma = \sqrt{\sigma^2}$.
* Nota: Se as observações forem medidas em metro, a **variância** terá unidade $\text{metro}^2$ o que comumente não estamos acostumados a interpretar. O **desvio padrão**, por outro lado, tem a mesma unidade de medida que as observações.

### Exemplo: número de caras em 3 lançamentos de uma moeda

A distribuição da variável aleatória 'número de caras em 3 lançamentos de uma moeda' tem variância
    $$ \sigma^2
    = (0-1.5)^2\frac{1}{8} + (1-1.5)^2\frac{3}{8} + (2-1.5)^2 \frac{3}{8} + (3-1.5)^2 \frac{1}{8} = 0.75.
    $$

e desvio padrão
    $$  \sigma = \sqrt{\sigma^2} = \sqrt{0.75} = 0.866.  $$


## A distribuição binomial


* A **distribuição binomial** é uma distribuição discreta.
* Seja $Y$ a variável aleatória que representa o número de sucessos obtidos em $n$ experimentos aleatórios (independentes). Assuma que cada experimento tem apenas dois resultados possíveis, denominados **sucesso** e **fracasso** e que cada experimento tem a mesma probabilidade $p$ de sucesso.
* Dizemos que $Y$ tem uma **distribuição binomial** com parâmetros $n$ e $p$.  
* Neste caso, podemos mostrar que
    $$p_Y(y) = P(Y = y) = \binom{n}{y} p^y (1-p)^{n-y},$$
    onde $\binom{n}{y}=\frac{n!}{y!(n-y)!}$ e $m!$ é o produto dos primeiros $m$ inteiros.
* Valor esperado: $\mu = n p$.
* Variância: $\sigma^2 = n p (1-p)$.
* Desvio padrão: $\sigma = \sqrt{n p (1-p)}$.


```{r dbinom}
# A distribuição binomial com n = 10 e p = 0.35:
plotDist("binom", size = 10, prob = 0.35,
         ylab = "Probabilidade", xlab = "Número de sucessos", main = "binom(n = 10, prob = 0.35)")
```



## Distribuição de uma variável aleatória contínua

* A distribuição de uma variável aleatória contínua $Y$ é caracterizada pela
  função densidade de probabilidade $f_Y$.

```{r normprobs,echo=FALSE,fig.width=6,fig.height=4}
x <- (-70:70)/20
par(mar=c(3,0,0,0))
plot(x, dnorm(x), axes=F, xlab="", ylab="", type="l", ylim=c(-.01,.4), main="")
abline(h=0)
lines(c(0,0),c(0,dnorm(0)))
lines(c(1.5,1.5),c(0,dnorm(1.5)))
x <- (0:30)/20
y <- c(0,dnorm(x),0)
polygon(c(0,x,1.5),y,density=20)#,col="cyan")
axis(1,at=0,labels="a",pos=0,cex.axis=1.5)
axis(1,at=1.5,labels="b",pos=0,cex.axis=1.5)
```

* A área sob o gráfico da função densidade de probabilidade entre $a$ e $b$
  é igual a probabilidade de uma observação neste intervalo.
* $f_Y(y)\geq 0$ para todos os números reais $y$.
* A área sob o gráfico de $f_Y$ é igual a 1.
* Por exemplo, a **distribuição uniforme** de $a$ até $b$:
    $$
    f_Y(y)=
    \begin{cases}
      \frac{1}{b-a} & a<y<b \\
      0 & \text{otherwise}
    \end{cases}
    $$
```{r unifdist,echo=FALSE,fig.width=6,fig.height=4}
par(mar=c(3,3,0,0))
plot(c(1,2,2,4,4,5)-1, c(0,0,.5,.5,0,0), axes=F, xlab="", ylab="", type="l", main="", lwd = 3, ylim = c(0,.6))
lines(c(0,0),c(0,.6))
lines(c(0,4.1),c(0,0))
axis(1,at=1,labels="a",pos=0,cex.axis=1.5)
axis(1,at=3,labels="b",pos=0,cex.axis=1.5)
axis(2,at=0,pos=0,cex.axis=1.5)
axis(2,at=.5,labels=expression(frac(1,b-a)),pos=0,cex.axis=1.5)
```


## Função Densidade
### Aumentando o número de observações

* Outra maneira de pensar sobre a densidade é em termos do histograma.
* Se desenharmos um histograma para uma amostra onde a área de cada caixa corresponde a frequência relativa de cada intervalo, a área total será $1$.
* Quando o número de observações (tamanho da amostra) aumenta nós podemos fazer intervalos menores e obter um histograma mais suave.
* Com um número infinito de observações, nós poderíamos produzir uma curva suave, onde a área embaixo dela é $1$.\ Uma função derivada dessa forma é o que nós chamamos de **função densidade de probabilidade**.

```{r histToPop,echo=FALSE,results='hide',fig.width=10,fig.height=4}
par(mfrow=c(1,3),cex.main = 2,cex.lab = 2,mar=c(5,5,4,1))
set.seed(100)
varValue <- rnorm(50,10,2)
hist(varValue,breaks="FD",ylab="Densidade",xlab = "y",ylim=c(0,.25),freq=F,main="Histograma de 50 obs.")
text(7,.22,bquote(bar(y) == .(round(mean(varValue),2))),cex=1.5)
text(14,.22,bquote(s == .(round(sd(varValue),2))),cex=1.5)
varValue <- rnorm(1000,10,2)
hist(varValue,breaks="FD",freq=F,ylab="Densidade",xlab = "y",ylim=c(0,.25),main="Histograma de 500 obs.")
text(7,.22,bquote(bar(y) == .(round(mean(varValue),2))),cex=1.5)
text(14,.22,bquote(s == .(round(sd(varValue),2))),cex=1.5)
varValue <- (20:180)/10
plot(varValue,dnorm(varValue,10,2),ylab="Densidade",xlab = "y",ylim=c(0,.25),type="l",main="Histograma da população")
text(7,.22,bquote(mu == 10),cex=1.5)
text(14,.22,bquote(sigma == 2),cex=1.5)
```

----

### Formatos das densidades

```{r densities, echo=FALSE, results='hide', fig.width=8, fig.height=4, out.width='\\textwidth'}
par(mfrow=c(2,2), cex.lab = 1, cex.main = 1, mar=c(1,5,4,1))

#x <- (0:200)/100
#y <- .5+(x-1)^2
#plot(x,y,axes=F,type="l",ylab="Densidade",xlab = "")

x <- seq(0, 1, length.out = 100)
plot(x,dbeta(x, 1/2, 1/2),axes=F,type="l",ylab="Densidade",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Densidade simétrica em forma de \n U")

x <- (0:200)/100
plot(x,dnorm(x,1,.35),axes=F,type="l",ylab="Density",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Densidade simétrica em forma de \n sino")

x <- (0:400)/100
plot(x,dgamma(x,1.5,1.5),axes=F,type="l",ylab="Densidade",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Densidade assimétrica à direita")

plot(x,dgamma(rev(x),1.5,1.5),axes=F,type="l",ylab="Densidade",xlab = "")
axis(1,labels=F)
axis(2,labels=F)
title("Densidade assimétrica à esquerda")
```

## Distribuição Normal

* A distribuição Normal é uma distribuição contínua determinada por 2 parâmetros:
    * $\mu$: a **média** (valor esperado), que determina
        onde a distribuição é centrada.
    * $\sigma^2$ a **variância**, que  determina
        a dispersão da distribuição em torno da média.
* A distribuição tem uma função densidade de probabilidade em forma de sino:
    $$ f_Y(y;\mu,\sigma^2) = \frac{1}{\sqrt{2p\sigma^2}}\exp \left (-\frac{1}{2\sigma^2}(y-\mu)^2 \right ) $$
* Quando uma variável aleatória $Y$ segue uma distribuição normal com média $\mu$ e variância
  $\sigma^2$, então nós escrevemos que $Y \sim \texttt{N}(\mu,\sigma^2)$.
* Chamamos de distribuição **Normal padrão** uma distribuição Normal com média 0 e variância $1$ e notamos isto por $Z \sim \texttt{N}(0, 1)$.

----

### Alcance da distribuição normal

```{r normalreach,echo=FALSE, fig.width=10, fig.height=5}
x <- (-70:70)/20
par(xpd=TRUE)
plot(x,dnorm(x),axes=F,xlab="",ylab="",type="l",ylim=c(-.21,.4), main="Densidade da distribuição normal", cex.main = 1.5)
axis(1,at=-3:3,labels=F,pos=0)
axis(1,at=-3,labels=substitute(mu-3*sigma),pos=0)
axis(1,at=-2,labels=substitute(mu-2*sigma),pos=0)
axis(1,at=-1,labels=substitute(mu-sigma),pos=0)
axis(1,at=0,labels=substitute(mu),pos=0)
axis(1,at=1,labels=substitute(mu+sigma),pos=0)
axis(1,at=2,labels=substitute(mu+2*sigma),pos=0)
axis(1,at=3,labels=substitute(mu+3*sigma),pos=0)
arrows(-1,-.1,1,-.1,col="red",code=3,length=.1)
text(-.01,-.115,"68%",col="red")
arrows(-2,-.15,2,-.15,col="blue",code=3,length=.1)
text(-.01,-.1655,"95%",col="blue")
arrows(-3,-.2,3,-.2,col="green",code=3,length=.1)
text(-.01,-.215,"99.7%",col="green")
text(0,-.28, substitute(paste("média ",mu," e desvio padrão ",sigma)), cex=1.5)
```

Interpretação:

* $\approx$ 68\% da população está dentro de um desvio padrão da média.
* $\approx$ 95\% da população está dentro de 2 desvios padrão da média.
* $\approx$ 99.7\% da população está dentro de 3 desvios padrão da média.

----

### Escore $z$
* Se $Y\sim \texttt{N}(\mu,\sigma^2)$ então o escore-$z$ correspondente é
  $$Z=\frac{Y-\mu}{\sigma}=\frac{\mathtt{observação-média}}{\mathtt{desvio\
      padrão}}$$
*  I.e. $Z$ representa o número de desvios padrão da
  observação em relação à média.
* $Z\sim \texttt{N}(0,1)$, i.e. $Z$ tem média zero
   e variância $1$.
* Isto implica que
    * $Z$ situa-se entre $-1$ e $1$ com probabilidade de 0.6826
    * $Z$ situa-se entre $-2$ e $2$ com probabilidade de 0.9544
    * $Z$ situa-se entre $-3$ e $3$ com probabilidade de 0.9973
* Isto também implica que:
    * A probabilidade de $Y$ estar entre $\mu - z\sigma$ e $\mu + z\sigma$
      é igual a probabilidade de $Z$ estar entre $-z$ e $z$.

----

### Calculando probabilidades na distribuição normal padrão

* A função `pdist` produz a área à esquerda do valor $z$ (quantil/percentil) que informamos (variável `q` na função), i.e. mostra a probabilidade de obter um valor menor do que $z$. O primeiro argumento de `pdist` denota a distribuição que estamos considerando.

```{r}
#Para uma distribuição normal padrão, a probabilidade de obter um valor menor que 1 é:
left_prob <- pdist("norm", q = 1, mean = 0, sd = 1)
left_prob
```

<!-- * Here there is a conflict between **R** and the textbook, since in the book we always consider right probabilities in the normal distribution. Since the total area is 1 and we have the left probability we easily get the right probability: -->

```{r}
right_prob <- 1 - left_prob
right_prob
```

* Para $z=1$ nós temos uma probabilidade à direita de $p=0.1587$, então a probabilidade de uma observação entre $-1$ e $1$ é $1 - 2 \cdot 0.1587 = 0.6826 = 68.26\%$ devido a simetria.

----

<!-- ### Calculando valores-$z$ (quantis) na distribuição normal padrão -->

<!-- * If we have a probability and want to find the corresponding $z$-value we again need to decide on left/right probability. The default in **R** is to find the left probability, so if we want the $z$-value with por exemplo, 0.5% probability to the left we get: -->
<!-- ```{r} -->
<!-- left_z <- qdist("norm", p = 0.005, mean = 0, sd = 1, xlim = c(-4, 4)) -->
<!-- left_z -->
<!-- ``` -->



<!-- * However, in all the formulas in the course we follow the textbook and consider $z$-values for a given right probability.\ por exemplo, with 0.5% probability to the right we get: -->
<!-- ```{r} -->
<!-- right_z <- qdist("norm", p = 1-0.005, mean = 0, sd = 1, xlim = c(-4, 4)) -->
<!-- right_z -->
<!-- ``` -->

<!-- * Thus, the probability of an observation between $-2.576$ and $2.576$ equals $1 - 2 \cdot 0.005 = 99\%$. -->

<!-- ---- -->

### Exemplo
  A escala de inteligência de Stanford-Binet é calibrada para ser aproximadamente normal
  com média 100 e desvio padrão 16.

  Qual é o  99-percentil dos escores de QI?

* O correpondente escore-$z$ é $Z=\frac{IQ-100}{16}$, o que
    significa que $QI=16Z+100$.
* O 99-percentil dos escores-$z$ é 2.326 (pode ser calculado usando `qdist`).
* Então, o 99-percentil dos escores de QI é:
  $$
  QI=16\cdot 2.326+100=137.2.
  $$
* Então, esperamos que uma a cada cem pessoas tenha um QI superior a 137.


# Distribuição da estatística amostral

## Estimativas e sua variabilidade

  Temos uma amostra $y_1,y_2,\ldots,y_n$.

* A média amostral $\bar{y}$ é a estimativa mais comum
  da média populacional $\mu$.
* O desvio padrão amostral, $s$, é a estimativa mais comum
  do desvio padrão populacional $\sigma$.
  Note que há uma incerteza (de amostra para amostra) conectada a
  estas estatísticas e, portanto, estamos interessados em descrever a sua **distribuição**.


## Distribuição da média amostral
* Temos uma amostra $y_1,y_2,\ldots,y_n$ de uma população com
  média $\mu$ e desvio padrão $\sigma$.
* A média amostral $$\bar{y}=\frac{1}{n}(y_1+y_2+\ldots+y_n)$$ tem
  distribuição
    + com média $\mu$,
    + e desvio padrão
        $\frac{\sigma}{\sqrt{n}}$ (também chamado
        de **erro padrão**), e
    + quando $n$ cresce, a distribuição se aproxima de uma distribuição
        normal. Este resultado é provado usando o **teorema central do limite**.

----

### Teorema Central do Limite
*  Os pontos anteriores podem ser resumidos por   $$
  \bar{y}\sim \texttt{N}\bigg(\mu,\frac{\sigma^{2}}{{n}}\bigg),
  $$ i.e. $\bar{y}$ tem distribuição aproximadamente
  normal com média $\mu$ e erro padrão
  $\frac{\sigma}{\sqrt{n}}$.
* Quando a amostra é suficientemente grande (para que a aproximação seja boa)
 isto nos permite fazer as seguintes observações:
    * Nós estamos $95\%$ certos de que $\bar{y}$ está no intervalo de
        $\mu-2\frac{\sigma}{\sqrt{n}}$ a $\mu+2\frac{\sigma}{\sqrt{n}}$.
    * Estamos quase completamente certos de que $\bar{y}$ está no intervalo
        $\mu-3\frac{\sigma}{\sqrt{n}}$ a $\mu+3\frac{\sigma}{\sqrt{n}}$.


<!-- ---- -->

### Ilustração do TCL
<!--    The central limit theorem is illustrated in Agresti: -->

![Teorema Central do Limite](https://asta.sfx.aau.dk/static-files/asta/img/AgrestiCLT.png)



----

### Exemplo

* Índice de Massa Corporal (IMC) de pessoas do norte da europa (2010) tem média
  $\mu=25.8$ kg/$\mathtt{m}^2$ e desvio padrão $4.8$ kg/$\mathtt{m}^2$.
* Uma amostra aleatória de $n=100$ clientes de uma hamburgueria teve um IMC médio
  dado por $\bar{y}=27.2$.
* Se comer hamburguer "não influencia" o IMC (e a amostra
  é representativa da população de interesse), então
  $$\bar{y} \approx \texttt{N}\bigg(\mu,\frac{\sigma^2}{n}\bigg)=\texttt{N}(25.8,0.48^2).$$
* Para amostra o escore-$z$ observado
  $$z_{obs}=\frac{27.2-25.8}{0.48}=2.92$$
* Lembrando que o escore-$z$ é (aproximadamente) normal padrão,
  a probabilidade de obter uma pontuação mais alta que o escore-$z$ é:

```{r}
1 - pdist("norm", mean = 0, sd = 1, q = 2.92, xlim = c(-4, 4))
```

* Assim, é altamente imporvável obter uma amostra aleatória com um
  escore-$z$ tão alto. Há evidências de que os clientes da hamburgueira tem um IMC médio
  maior que a média populacional.



```{r, include = FALSE}
# Waiting times
# set.seed(1)
# y_canteen_tmp <- y_canteen[y_canteen < 20]
# y <- cbind(y_canteen_tmp, replicate(4, sample(y_canteen_tmp, replace = TRUE)))
# hist(apply(y, 1, mean), breaks = "FD")
```

